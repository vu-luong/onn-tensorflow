{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45312, 9)\n",
      "(45312, 8)\n",
      "(45312,)\n",
      "n_classes =  2\n",
      "n_features =  8\n"
     ]
    }
   ],
   "source": [
    "filepath = '/Users/AnhVu/Study/PhD/mypaper/online_deep_forest/data/csv/electricity-normalized.csv'\n",
    "dataset = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "print(dataset.shape)\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "n_classes = np.unique(y).shape[0]\n",
    "n_features = X.shape[1]\n",
    "print('n_classes = ', n_classes)\n",
    "print('n_features = ', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONN(tf.keras.Model):\n",
    "    def __init__(self, n_features, n_classes,\n",
    "                n_hidden_units=10, beta=0.99,\n",
    "                learning_rate=0.01, s=0.2,\n",
    "                n_layers=20):\n",
    "        super(ONN, self).__init__(name='ONN')\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_layers = n_layers\n",
    "        self.beta = beta\n",
    "        self.s = s\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            self.hidden_layers.append(\n",
    "                Dense(n_hidden_units)\n",
    "            )\n",
    "            \n",
    "        for i in range(n_layers + 1):\n",
    "            self.output_layers.append(\n",
    "                Dense(n_classes)\n",
    "            )\n",
    "            \n",
    "        self.alphas = tf.Variable(\n",
    "            np.ones(n_layers + 1) * (1./(n_layers + 1)),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        hidden_connections = []\n",
    "        x = inputs\n",
    "        hidden_connections.append(x)\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            hidden_connections.append(\n",
    "                tf.nn.relu(\n",
    "                    self.hidden_layers[i](\n",
    "                        hidden_connections[i]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        output_class = []\n",
    "        for i in range(self.n_layers + 1):\n",
    "            output_class.append(\n",
    "                self.output_layers[i](\n",
    "                    hidden_connections[i]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return output_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.       1.       0.       0.056443 0.439155 0.003467 0.422915 0.414912]], shape=(1, 8), dtype=float32)\n",
      "21\n",
      "tf.Tensor([[-0.32111368 -0.26206058]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = ONN(n_features, n_classes)\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "v = tf.constant(X[0, :], shape=[1, n_features])\n",
    "print(v)\n",
    "\n",
    "print(len(model(v)))\n",
    "print(model(v)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(model, x, y):\n",
    "    output_class = model(x)\n",
    "    losses = []\n",
    "    for i in range(len(output_class)):\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        losses.append(\n",
    "            loss_object(y_true=y, y_pred=output_class[i])\n",
    "        )\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, targets, learning_rate):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        losses = loss(model, inputs, targets)\n",
    "    \n",
    "    # layer 0\n",
    "    dV0_weight, dV0_bias = tape.gradient(\n",
    "        losses[0], model.output_layers[0].variables\n",
    "    )\n",
    "    # update weight\n",
    "    model.output_layers[0].variables[0].assign_sub(\n",
    "        model.alphas[0] * learning_rate * dV0_weight\n",
    "    )\n",
    "    # update bias\n",
    "    model.output_layers[0].variables[1].assign_sub(\n",
    "        model.alphas[0] * learning_rate * dV0_bias\n",
    "    )\n",
    "    \n",
    "    w = []\n",
    "    b = []\n",
    "    \n",
    "    # layer 1,2,...,L\n",
    "    for i in range(1, len(losses)):\n",
    "        variables = []\n",
    "        for j in range(i):\n",
    "            variables.append(model.hidden_layers[j].variables)\n",
    "            \n",
    "        variables.append(model.output_layers[i].variables)\n",
    "        \n",
    "        grads = tape.gradient(\n",
    "            losses[i], variables\n",
    "        )\n",
    "        \n",
    "        # 1. For output layer\n",
    "        dVi_weight, dVi_bias = grads[-1]\n",
    "        \n",
    "        # update weight\n",
    "        model.output_layers[i].variables[0].assign_sub(\n",
    "            model.alphas[i] * learning_rate * dVi_weight\n",
    "        )\n",
    "        # update bias\n",
    "        model.output_layers[i].variables[1].assign_sub(\n",
    "            model.alphas[i] * learning_rate * dVi_bias\n",
    "        )\n",
    "        \n",
    "        # 2. For hidden layer\n",
    "        for j in range(i):\n",
    "            if len(w) < j + 1:\n",
    "                w.append(tf.Variable(model.alphas[i] * grads[j][0]))\n",
    "                b.append(tf.Variable(model.alphas[i] * grads[j][1]))\n",
    "            else:\n",
    "                w[j].assign_add(model.alphas[i] * grads[j][0])\n",
    "                b[j].assign_add(model.alphas[i] * grads[j][1])\n",
    "        \n",
    "    for i in range(len(losses) - 1):\n",
    "        model.hidden_layers[i].variables[0].assign_sub(\n",
    "            learning_rate * w[i]\n",
    "        )\n",
    "        \n",
    "        model.hidden_layers[i].variables[1].assign_sub(\n",
    "            learning_rate * b[i]\n",
    "        )\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        model.alphas[i].assign(\n",
    "            tf.math.pow(\n",
    "                model.beta, losses[i]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.alphas[i].assign(\n",
    "            tf.math.maximum(\n",
    "                model.alphas[i],\n",
    "#                 model.s / model.n_layers\n",
    "                0.01\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    z_t = tf.math.reduce_sum(model.alphas)\n",
    "    model.alphas.assign(model.alphas / z_t)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "1.0\n",
      "#100\n",
      "0.38613861386138615\n",
      "#200\n",
      "0.5174129353233831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9c83d26debce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#         print(model.alphas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0b5079f2ce0c>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         losses.append(\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m--> 128\u001b[0;31m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m         losses, sample_weight)\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Convert the result back to the input type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mreduce_weighted_loss\u001b[0;34m(weighted_losses, reduction)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM_OVER_BATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_num_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1573\u001b[0m       gen_math_ops._sum(\n\u001b[1;32m   1574\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11146\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11147\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keep_dims\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11148\u001b[0;31m         keep_dims)\n\u001b[0m\u001b[1;32m  11149\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11150\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ONN(n_features, n_classes, n_layers=0)\n",
    "\n",
    "# inputs = tf.constant(X[:, :], shape=[X.shape[0], n_features])\n",
    "# outputs = tf.constant(y[:], shape=[y.shape[0]])\n",
    "epochs = range(1)\n",
    "cnt = 0\n",
    "for epoch in epochs:\n",
    "    for i in range(X.shape[0]):\n",
    "\n",
    "        inputs = tf.constant(X[i, :], shape=[1, n_features])\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        prob = np.zeros(n_classes)\n",
    "        for k in range(len(outputs)):\n",
    "            t = model.alphas[k].numpy() * tf.nn.softmax(outputs[k])\n",
    "            prob = prob + t.numpy()\n",
    "        \n",
    "        preds = np.argmax(prob, axis=1)\n",
    "        pred = preds[0]\n",
    "        if pred == y[i]:\n",
    "            cnt += 1\n",
    "            \n",
    "        if (i % 100 == 0):\n",
    "            print('#{}'.format(i))\n",
    "            print(cnt / (i + 1))\n",
    "        \n",
    "        targets = tf.constant(y[i], shape=[1])\n",
    "        \n",
    "        current_loss = loss(model, inputs, targets)\n",
    "\n",
    "        train(model, inputs, targets, learning_rate=0.01)\n",
    "\n",
    "y_pred = model(inputs).numpy()\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = [1,2]\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
