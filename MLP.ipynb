{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45312, 9)\n",
      "(45312, 8)\n",
      "(45312,)\n",
      "n_classes =  2\n",
      "n_features =  8\n"
     ]
    }
   ],
   "source": [
    "filepath = '/Users/AnhVu/Study/PhD/mypaper/online_deep_forest/data/csv/electricity-normalized.csv'\n",
    "dataset = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "print(dataset.shape)\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "n_classes = np.unique(y).shape[0]\n",
    "n_features = X.shape[1]\n",
    "print('n_classes = ', n_classes)\n",
    "print('n_features = ', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(MLP, self).__init__(name='MLP')\n",
    "        self.dense_1 = Dense(32)\n",
    "        self.dense_2 = Dense(n_classes)\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.dense_2(x)\n",
    "        \n",
    "    \n",
    "model = MLP(n_features, n_classes)\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "# v = tf.constant(X[0, :], shape=[1, n_features])\n",
    "# print(v)\n",
    "\n",
    "# print(model(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# l = loss(model, X, y)\n",
    "# print('Loss test: {}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def train(model, inputs, targets, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "#         pdb.set_trace()\n",
    "\n",
    "#     dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "#     model.W.assign_sub(learning_rate * dW)\n",
    "#     model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "    dW1, dW2 = tape.gradient(loss_value, [model.dense_1.variables, model.dense_2.variables])\n",
    "    model.dense_1.variables[0].assign_sub(learning_rate * dW1[0])\n",
    "    model.dense_1.variables[1].assign_sub(learning_rate * dW1[1])\n",
    "    model.dense_2.variables[0].assign_sub(learning_rate * dW2[0])\n",
    "    model.dense_2.variables[1].assign_sub(learning_rate * dW2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch  0: loss=1.41875\n",
      "1\n",
      "Epoch  1: loss=1.32632\n",
      "2\n",
      "Epoch  2: loss=1.24084\n",
      "3\n",
      "Epoch  3: loss=1.16240\n",
      "4\n",
      "Epoch  4: loss=1.09111\n",
      "5\n",
      "Epoch  5: loss=1.02702\n",
      "6\n",
      "Epoch  6: loss=0.97012\n",
      "7\n",
      "Epoch  7: loss=0.92030\n",
      "8\n",
      "Epoch  8: loss=0.87726\n",
      "9\n",
      "Epoch  9: loss=0.84058\n",
      "10\n",
      "Epoch 10: loss=0.80971\n",
      "11\n",
      "Epoch 11: loss=0.78404\n",
      "12\n",
      "Epoch 12: loss=0.76294\n",
      "13\n",
      "Epoch 13: loss=0.74575\n",
      "14\n",
      "Epoch 14: loss=0.73183\n",
      "15\n",
      "Epoch 15: loss=0.72064\n",
      "16\n",
      "Epoch 16: loss=0.71167\n",
      "17\n",
      "Epoch 17: loss=0.70450\n",
      "18\n",
      "Epoch 18: loss=0.69879\n",
      "19\n",
      "Epoch 19: loss=0.69424\n",
      "20\n",
      "Epoch 20: loss=0.69062\n",
      "21\n",
      "Epoch 21: loss=0.68773\n",
      "22\n",
      "Epoch 22: loss=0.68544\n",
      "23\n",
      "Epoch 23: loss=0.68360\n",
      "24\n",
      "Epoch 24: loss=0.68214\n",
      "25\n",
      "Epoch 25: loss=0.68097\n",
      "26\n",
      "Epoch 26: loss=0.68003\n",
      "27\n",
      "Epoch 27: loss=0.67928\n",
      "28\n",
      "Epoch 28: loss=0.67867\n",
      "29\n",
      "Epoch 29: loss=0.67818\n",
      "30\n",
      "Epoch 30: loss=0.67778\n",
      "31\n",
      "Epoch 31: loss=0.67746\n",
      "32\n",
      "Epoch 32: loss=0.67719\n",
      "33\n",
      "Epoch 33: loss=0.67697\n",
      "34\n",
      "Epoch 34: loss=0.67679\n",
      "35\n",
      "Epoch 35: loss=0.67664\n",
      "36\n",
      "Epoch 36: loss=0.67651\n",
      "37\n",
      "Epoch 37: loss=0.67640\n",
      "38\n",
      "Epoch 38: loss=0.67630\n",
      "39\n",
      "Epoch 39: loss=0.67622\n",
      "40\n",
      "Epoch 40: loss=0.67615\n",
      "41\n",
      "Epoch 41: loss=0.67608\n",
      "42\n",
      "Epoch 42: loss=0.67603\n",
      "43\n",
      "Epoch 43: loss=0.67597\n",
      "44\n",
      "Epoch 44: loss=0.67592\n",
      "45\n",
      "Epoch 45: loss=0.67588\n",
      "46\n",
      "Epoch 46: loss=0.67583\n",
      "47\n",
      "Epoch 47: loss=0.67579\n",
      "48\n",
      "Epoch 48: loss=0.67575\n",
      "49\n",
      "Epoch 49: loss=0.67571\n",
      "50\n",
      "Epoch 50: loss=0.67568\n",
      "51\n",
      "Epoch 51: loss=0.67564\n",
      "52\n",
      "Epoch 52: loss=0.67561\n",
      "53\n",
      "Epoch 53: loss=0.67557\n",
      "54\n",
      "Epoch 54: loss=0.67554\n",
      "55\n",
      "Epoch 55: loss=0.67550\n",
      "56\n",
      "Epoch 56: loss=0.67547\n",
      "57\n",
      "Epoch 57: loss=0.67544\n",
      "58\n",
      "Epoch 58: loss=0.67540\n",
      "59\n",
      "Epoch 59: loss=0.67537\n",
      "60\n",
      "Epoch 60: loss=0.67534\n",
      "61\n",
      "Epoch 61: loss=0.67531\n",
      "62\n",
      "Epoch 62: loss=0.67527\n",
      "63\n",
      "Epoch 63: loss=0.67524\n",
      "64\n",
      "Epoch 64: loss=0.67521\n",
      "65\n",
      "Epoch 65: loss=0.67518\n",
      "66\n",
      "Epoch 66: loss=0.67514\n",
      "67\n",
      "Epoch 67: loss=0.67511\n",
      "68\n",
      "Epoch 68: loss=0.67508\n",
      "69\n",
      "Epoch 69: loss=0.67505\n",
      "70\n",
      "Epoch 70: loss=0.67502\n",
      "71\n",
      "Epoch 71: loss=0.67498\n",
      "72\n",
      "Epoch 72: loss=0.67495\n",
      "73\n",
      "Epoch 73: loss=0.67492\n",
      "74\n",
      "Epoch 74: loss=0.67489\n",
      "75\n",
      "Epoch 75: loss=0.67486\n",
      "76\n",
      "Epoch 76: loss=0.67482\n",
      "77\n",
      "Epoch 77: loss=0.67479\n",
      "78\n",
      "Epoch 78: loss=0.67476\n",
      "79\n",
      "Epoch 79: loss=0.67473\n",
      "80\n",
      "Epoch 80: loss=0.67470\n",
      "81\n",
      "Epoch 81: loss=0.67467\n",
      "82\n",
      "Epoch 82: loss=0.67464\n",
      "83\n",
      "Epoch 83: loss=0.67460\n",
      "84\n",
      "Epoch 84: loss=0.67457\n",
      "85\n",
      "Epoch 85: loss=0.67454\n",
      "86\n",
      "Epoch 86: loss=0.67451\n",
      "87\n",
      "Epoch 87: loss=0.67448\n",
      "88\n",
      "Epoch 88: loss=0.67445\n",
      "89\n",
      "Epoch 89: loss=0.67442\n",
      "90\n",
      "Epoch 90: loss=0.67438\n",
      "91\n",
      "Epoch 91: loss=0.67435\n",
      "92\n",
      "Epoch 92: loss=0.67432\n",
      "93\n",
      "Epoch 93: loss=0.67429\n",
      "94\n",
      "Epoch 94: loss=0.67426\n",
      "95\n",
      "Epoch 95: loss=0.67423\n",
      "96\n",
      "Epoch 96: loss=0.67420\n",
      "97\n",
      "Epoch 97: loss=0.67417\n",
      "98\n",
      "Epoch 98: loss=0.67414\n",
      "99\n",
      "Epoch 99: loss=0.67411\n",
      "(45312, 2)\n"
     ]
    }
   ],
   "source": [
    "model = MLP(n_features, n_classes)\n",
    "\n",
    "# input0 = tf.constant(X[:, :], shape=[X.shape[0], n_features])\n",
    "# output0 = tf.constant(y[:], shape=[y.shape[0]])\n",
    "\n",
    "# print(input0.numpy().shape)\n",
    "# print(output0.numpy().shape)\n",
    "# loss(model, input0, output0)\n",
    "\n",
    "inputs = tf.constant(X[:, :], shape=[X.shape[0], n_features])\n",
    "outputs = tf.constant(y[:], shape=[y.shape[0]])\n",
    "epochs = range(100)\n",
    "for epoch in epochs:\n",
    "    print(epoch)\n",
    "    current_loss = loss(model, inputs, outputs)\n",
    "    print('Epoch %2d: loss=%2.5f' %\n",
    "    (epoch, current_loss))\n",
    "    train(model, inputs, outputs, learning_rate=0.01)\n",
    "\n",
    "y_pred = model(inputs).numpy()\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.argmax(tf.nn.softmax(y_pred),axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5898658192090396"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_ == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=61, shape=(), dtype=float64, numpy=1.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "a = tf.Variable(np.ones(10))\n",
    "# a.assign([1., 2., 3.])\n",
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "a = [[1,3],[2,3]]\n",
    "\n",
    "x, y = a[1]\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
